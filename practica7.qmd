---
title: "Ejercicios para exponer"
subtitle: "20582- Análisis de Datos para el GMAT"
date: today
format:
  html:
    theme: lumen
    toc: true
    toc-depth: 3
Rendering:
    embed-resources: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Problema 5 - Enunciado

La tabla de [datos_antropométricos](https://github.com/igmuib/Practica_AD/blob/main/datos_antropometricos.csv) presenta un conjunto de datos simulados que recopila información sobre 200 observaciones de hombres y mujeres. Este conjunto incluye las siguientes variables:

-   `altura`: Altura en centímetros
-   `peso`: Peso en kilogramos
-   `cintura`: Circunferencia de la cintura en centímetros
-   `cadera`: Circunferencia de la cadera en centímetros
-   `IMC`: Índice de Masa Corporal (IMC)
-   `grasa_corporal`: Porcentaje de grasa corporal

Presentad un análisis exploratorio de los datos junto con un resumen de lo observado en el contexto del problema. Aplicad técnicas de clustering para agrupar individuos que tengan formas de cuerpos semejantes. Escribid una conclusión del análisis realizado en el contexto del problema

# Librerias y carga de datos

```{r}
library(readr) # Para lectura de datos csv.
library(dplyr)
library(tidyverse)
library(ggcorrplot) # Para la creación del gráfico de correlación
library(GGally) # Para comparar variables a pares (ggpairs)
library(scales) # Para editar gráficos (cambiar ancho del histograma)
library(factoextra) # Para hacer el mapa de calor y usar el metodo del codo 
library(cluster)

```

```{r}
datos <- read_csv("datos_antropometricos.csv")
head(datos)
datos_numericos<-datos %>% select(2:7)
```

# Analisis descriptivo de los datos

```{r}
ggpairs(
  datos, 
  aes(color = factor(sexo), alpha = 0.7),
  upper = list(continuous = wrap("cor", size = 3)),  # Tamaño del texto de correlación
  diag = list(continuous = wrap("densityDiag")),
  lower = list(continuous = wrap("points", size = 0.5))
) +
  scale_color_brewer(palette = "Set2", name = "Sexo") +
  labs(
    title = "Gráfico de Pares por Sexo",
    subtitle = "Relación entre variables en función del sexo"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 10, hjust = 0.5),
    strip.text = element_text(size = 8),
    axis.text = element_text(size = 7)
  )

```

Analicemos primero las variables de forma individual.

-   Todas las variables parecen seguir una distribución normal.
-   En algunas variables, observamos la presencia de algunos valores bastante superiores o inferiores que el resto (outliers). Lo deberemos tener en cuenta próximamente para realizar el clustering.
-   Al separar los datos en función del sexo, observamos que en la **altura**, **peso** y **cintura**, la mujeres tienen valores superiores. Para la **cardera** y el **IMC** tienen valores similares. Y para la **grasa corporal** los hombres tienen valores superiores.

En cuanto al análisis por pares tenemos

-   En las variables **altura**, **peso** y **cintura** observamos que los datos estan agrupados por sexo. Posteriormente, en el clustering, lo analizaremos.
-   En las correlaciones, destacan
    - La correlación positiva entre: **peso** y **altura**; **cintura** y **altura** ; **cintura** y **peso**.
    - La correlación negativa entre: **grasa corporal** y **altura**; **grasa corporal** y **peso** ; **grasa corporal** y **cintura**.
    - Mencionar que sorprende la baja correlación del **IMC** con la **altura** y el **peso**, ya que el IMC se calcula siguiendo la fórmula $IMC = \frac{peso}{altura^2}$.
    
```{r}
# Calcular la matriz de distancia
distancia <- dist(datos_numericos, method = "euclidean")  # Cambia "euclidean" si prefieres otro método

# Visualizar el mapa de calor de distancias
fviz_dist(as.dist(distancia), 
          gradient = list(low = "blue", mid = "white", high = "red"),
          lab_size = 8) +
  ggtitle("Mapa de Calor de Distancias") +
  theme_minimal()+
  theme(
    axis.text.x = element_blank(), # Quitar texto del eje X
    axis.text.y = element_blank()
  )
```


```{r}
# Calcular la matriz de correlación
matriz_correlacion <- cor(datos %>%
                            select("altura","peso","cintura","cadera","IMC","grasa_corporal")
                          )

# Generar el gráfico de correlación
ggcorrplot(matriz_correlacion, 
           hc.order = TRUE,         # Ordenar jerárquicamente
           type = "lower",          # Mostrar solo la mitad inferior
           lab = TRUE,              # Añadir los valores de correlación
           lab_size = 3.5,          # Tamaño del texto de los valores
           colors = c("#4575b4", "#f7f7f7", "#d73027")) +  # Colores personalizados
  ggtitle("Mapa de Correlación de Variables") +    # Añadir título
  labs(x = "Variables", y = "Variables") +         # Títulos de los ejes
  theme(plot.title = element_text(hjust = 0.5, size = 16),   # Centrar el título y ajustar tamaño
        axis.title.x = element_text(size = 14),   # Tamaño del título del eje x
        axis.title.y = element_text(size = 14))   # Tamaño del título del eje y

```

**Metodo codo+ visualtización de cluster con diferentes distancias**

Similar al ejemplo 5.2.5 al tener datos con procedemos a escalarlos, usaremos el método del codo para obtener el número de clusters óptimo. Primero probaremos usando la distáncia euclidea.

```{r}
escalado<-scale(datos_numericos)
fviz_nbclust(x = escalado, FUNcluster = kmeans, method = "wss",
             diss = dist(datos, method = "euclidean")) +
  geom_vline(xintercept = 4, linetype = 2)
```

Parece razonable pues escoger $k=4$ como número de clusters, ahora fijemos una semilla i veamos si los datos se comportan bien con este número de clusters. También calcularemos los puntos iniciales del algoritmo.

```{r}
set.seed(10)
centro_cluster<-kmeans(x=escalado,centers = 4,nstart = 25)
```

Al tener en total 6 variables el código reducirá la dimensión de estos usando sus dos primeras componenetes principales.

```{r}
fviz_cluster(object = centro_cluster, data = escalado, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  theme_bw() + theme(legend.position = "none")
```

De manera directa podemos ver que entre el verde y rojo hay muchissimos valores solapados, lo que nos indica que un valor menor de clusters implicará un mejor resultado, al haber intersecciones tanto en los clusters azul-lila como verde-rojo veamos como resulta el clustering con $k=2$.

```{r}
centro_cluster_1<-kmeans(x=escalado,centers = 2,nstart = 25)
fviz_cluster(object = centro_cluster_1, data = escalado, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  theme_bw() + theme(legend.position = "none")
```

Este clustering nos definiria dos tipos de cuerpo pero debemos notar que la variabilidad definida por las componenetes principales es $56.8\%<75\%$ por lo que no seria un modelo muy fiable. Esta falta de variabilidad puede ser debido al uso de k-medias o la distancia euclidiana, como podemos notar hay muchos valores lejanos al centro del cluster, en este caso será mas robusto el uso de k-medoides.

Al tener valores lejanos el metodo "manhattan" resultará mas apto para el cálculo del número de clusters

```{r}
fviz_nbclust(x = escalado, FUNcluster = pam, method = "wss",
             diss = dist(datos, method = "manhattan"))
```

Para encontrar los clusters por k-medoides usaremos la función "pam" con $k=2$, de nuevo usamos la metrica "manhattan" por la presencia de dispersión entre valores.

```{r}
medoide_clusters <- pam(x = escalado, k = 2, metric = "manhattan")
medoide_clusters
```

Aqui falta fer una explicació del que es veu aquí!!!!!!!!!!!!

Veamos una representación gráfica del clustering dado por la función "pam":

```{r}
fviz_cluster(object = medoide_clusters, data = escalado, ellipse.type = "t", repel = TRUE) +
  theme_bw() + theme(legend.position = "none")
```

Veamos si obviando algunas variables podemos encontrar un modelo que tenga mayor representación de variabilidad en las componenetes principales. Debido a la alta relación de las variables podemos ver con la matriz de corelaciones que las varaibles con mas peso son altura,peso y cintura. Por ejemplo el IMC está completamente definido por el peso y la altura.

```{r}
mios<-datos %>% select("altura","peso","cintura")
fviz_nbclust(x = mios, FUNcluster = pam, method = "wss",
             diss = dist(datos, method = "manhattan"))
```

Por el metodo del codo el valor $k=2$ dará los mejores resultados además de evitar solapamientos como antes.

```{r}
medoide_clusters <- pam(x = mios, k = 2, metric = "manhattan")
medoide_clusters
fviz_cluster(object = medoide_clusters, data = mios, ellipse.type = "t", repel = TRUE) +
  theme_bw() + theme(legend.position = "none")
```
Así obtenemos una variabilidad total de $83.6\%$ con un solapamiento tolerable comparado con los valores de $k$ mayores.

```{r}
medoide_clusters
```


```{r}
datos <- datos %>%
  mutate(cluster = medoide_clusters$clustering)

# Crear tabla de contingencia
tabla_contingencia <- table(datos$sexo, datos$cluster)

# Ver la tabla
print(tabla_contingencia)

```
Por hacer:
-   **Explicar que el cluster ha sido capaz de separar entre hombres y mujeres**
-   **Analizar que les pasa a los individuos con no se han separado (Ej. quizá es un hombre más alto que el resto)**


