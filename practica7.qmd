---
title: "Problema 5"
subtitle: "20582- Análisis de Datos para el GMAT"
author: Biel Bauzà, Marc Arrom, Eulàlia Tous i Rebeca Payà
date: today
format:
  html:
    theme: lumen
    toc: true
    toc-depth: 3
Rendering:
    embed-resources: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Enunciat

La tabla de [datos_antropométricos](https://github.com/igmuib/Practica_AD/blob/main/datos_antropometricos.csv) presenta un conjunto de datos simulados que recopila información sobre 200 observaciones de hombres y mujeres. Este conjunto incluye las siguientes variables:

-   `altura`: Altura en centímetros
-   `peso`: Peso en kilogramos
-   `cintura`: Circunferencia de la cintura en centímetros
-   `cadera`: Circunferencia de la cadera en centímetros
-   `IMC`: Índice de Masa Corporal (IMC)
-   `grasa_corporal`: Porcentaje de grasa corporal

Presentad un análisis exploratorio de los datos junto con un resumen de lo observado en el contexto del problema. Aplicad técnicas de clustering para agrupar individuos que tengan formas de cuerpos semejantes. Escribid una conclusión del análisis realizado en el contexto del problema

```{r, echo=FALSE}
library(readr) # Para lectura de datos csv.
library(dplyr)
library(tidyverse)
library(ggcorrplot) # Para la creación del gráfico de correlación
library(GGally) # Para comparar variables a pares (ggpairs)
library(scales) # Para editar gráficos (cambiar ancho del histograma)
library(factoextra) # Para hacer el mapa de calor y usar el metodo del codo 
library(cluster)

```

```{r, include=FALSE}
datos <- read_csv("datos_antropometricos.csv")
head(datos)
datos_numericos<-datos %>% select(2:7)
```

# Análisis descriptivo de los datos

[Enlace al GitHub](https://github.com/BielBLl/Practica_cluster)

En este apartado trataremos de hacer una visualización general de los datos y su comportamiento entre ellos, como por ejemplo estudiar su correlación y medias separadas por sexo. A continuación, mostraremos un par de gráficos donde podremos observar el comportamiento de nuestros datos de una forma más intuitiva y visual.

```{r, echo=FALSE}
# Calcular la matriz de correlación
matriz_correlacion <- cor(datos %>%
                            select("altura","peso","cintura","cadera","IMC","grasa_corporal")
                          )

# Generar el gráfico de correlación
ggcorrplot(matriz_correlacion, 
           hc.order = TRUE,         # Ordenar jerárquicamente
           type = "lower",          # Mostrar solo la mitad inferior
           lab = TRUE,              # Añadir los valores de correlación
           lab_size = 3.5,          # Tamaño del texto de los valores
           colors = c("#4575b4", "#f7f7f7", "#d73027")) +  # Colores personalizados
  ggtitle("Mapa de Correlació de Variables") +    # Añadir título
  labs(x = "Variables", y = "Variables") +         # Títulos de los ejes
  theme(plot.title = element_text(hjust = 0.5, size = 16),   # Centrar el título y ajustar tamaño
        axis.title.x = element_text(size = 14),   # Tamaño del título del eje x
        axis.title.y = element_text(size = 14))   # Tamaño del título del eje y

```


Podemos observar que las variables de altura y cintura tienen una correlación positiva muy alta respecto al resto. Otras variables que también tienen una alta correlación positiva son el peso con la altura y la cintura, algo que tiene bastante sentido. Por otro lado, podemos ver que el indice de grasa corporal tiene una alta correlación negativa con el peso, la altura y la cintura.

```{r, echo =FALSE}
ggpairs(
  datos, 
  aes(color = sexo, alpha = 0.8),
  upper = list(continuous = wrap("cor", size = 3.1)),  # Tamaño del texto de correlación
  diag = list(continuous = wrap("densityDiag")),
  lower = list(continuous = wrap("points", size = 0.5))
) +
  labs(
    title = "Gráfico de Pares por Sexo",
    subtitle = "Relación entre variables en función del sexo"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 10, hjust = 0.5),
    strip.text = element_text(size = 8),
    strip.background = element_rect(fill = "grey90", color = "grey80", alpha(0.3)), # Fondo gris y borde
    axis.text = element_text(size = 6.9),
    axis.text.x = element_text(size = 5.3)
  )
```
Analizamos primero las variables de forma individual.

-   Podemos observar que la mayoría parecen seguir una distribución normal aproximada, pero sería necesario realizar contrastes de hipótesis para poder afirmarlo.
-   Separando los datos en función del sexo, observamos que la **altura**, el **peso** y la **cintura** tienen valores más altos en el caso de los hombres, mientras que la mayoría de mujeres tienen un **porcentaje de grasa corporal** superior. Los valores de la **cadera** y del **IMC** son similares en ambos casos, aunque la media sigue siendo mayor en el caso de las mujeres.

En cuanto al análisis por pares tenemos

-   En las variables **altura**, **peso** y **cintura** observamos que los datos estan agrupados por sexo. Posteriormente, en el clustering, lo analizaremos.
-   En las correlaciones, destacan
    - La correlación positiva entre: **peso** y **altura**; **cintura** y **altura** ; **cintura** y **peso**.
    - La correlación negativa entre: **grasa corporal** y **altura**; **grasa corporal** y **peso** ; **grasa corporal** y **cintura**.
    - Mencionar que sorprende la baja correlación del **IMC** con la **altura** y el **peso**, ya que el IMC se calcula siguiendo la fórmula $IMC = \frac{peso}{altura^2}$.
    
```{r, echo=FALSE}
# Calcular la matriz de distancia
distancia <- dist(datos_numericos, method = "manhattan")  # Cambia "euclidean" si prefieres otro método

# Visualizar el mapa de calor de distancias
fviz_dist(as.dist(distancia), 
          gradient = list(low = "blue", mid = "white", high = "red"),
          lab_size = 8) +
  ggtitle("Mapa de Calor de Distancias") +
  theme_minimal()+
  theme(
    axis.text.x = element_blank(), # Quitar texto del eje X
    axis.text.y = element_blank()
  )
```

Las celdas azules indican observaciones con baja distancia, es decir, son valores similares entre sí. Las celdas rojas muestran observaciones con grandes distancias, es decir, valores diferentes entre sí. Las celdas blancas representan valores intermedios de distancia.

Se observan bloques azules a lo largo de la diagonal, esto indica la presencia de grupos de observaciones similares, lo que quiere decir que pueden ser posibles clústeres.

## Método del codo y visualización de cluster con diferentes distancias

Similar al ejemplo 5.2.5 al tener datos numéricos muy diferentes procedemos a escalarlos, usaremos el método del codo para obtener el número de clusters óptimo. Primero probaremos usando la distáncia euclidea.

```{r, echo=FALSE}
escalado<-scale(datos_numericos)
fviz_nbclust(x = escalado, FUNcluster = kmeans, method = "wss",
             diss = dist(datos, method = "euclidean")) +
  geom_vline(xintercept = 4, linetype = 2)
```

Parece razonable pues escoger $k=4$ como número de clusters, ahora fijemos una semilla y veamos si los datos se comportan bien con este número de clusters. También calcularemos los puntos iniciales del algoritmo.

```{r, echo=FALSE}
set.seed(10)
centro_cluster<-kmeans(x=escalado,centers = 4,nstart = 25)
```

Al tener en total 6 variables el código reducirá la dimensión de estos usando sus dos primeras componenetes principales.

```{r, echo=FALSE}
fviz_cluster(object = centro_cluster, data = escalado, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  theme_bw() + theme(legend.position = "none")
```

De manera directa podemos ver que entre el verde y azul hay muchíssimos valores solapados, lo que nos indica que un valor menor de clusters implicará un mejor resultado, al haber intersecciones tanto en los clusters rojo-lila como verde-azul veamos como resulta el clustering con $k=2$.

```{r, echo=FALSE}
centro_cluster_1<-kmeans(x=escalado,centers = 2,nstart = 25)
fviz_cluster(object = centro_cluster_1, data = escalado, show.clust.cent = TRUE,
             ellipse.type = "euclid", star.plot = TRUE, repel = TRUE) +
  theme_bw() + theme(legend.position = "none")
```
Este clustering nos definiria dos tipos de cuerpos, pero debemos notar que la variabilidad definida por las componenetes principales es $56.8\%<75\%$, por lo que no seria un modelo muy fiable. Esta falta de variabilidad puede ser debido al uso de k-medias o la distancia euclidiana, como podemos notar hay muchos valores lejanos al centro del cluster, en este caso será mas robusto el uso de k-medoides, el qual tiene mejor en cuenta estos "outliers".

Al tener valores lejanos, el metodo "manhattan" resultará mas apto para el cálculo del número de clusters.

```{r, echo=FALSE}
fviz_nbclust(x = escalado, FUNcluster = pam, method = "wss",
             diss = dist(datos, method = "manhattan"))
```

Para encontrar los clusters por k-medoides usaremos la función "pam" con $k=2$, de nuevo usamos la metrica "manhattan" por la presencia de "outliers".

```{r, echo=FALSE}
medoide_clusters <- pam(x = escalado, k = 2, metric = "manhattan")
medoide_clusters
```

El primer bloque muestra los medoides, los puntos centrales de cada clúster. Estos puntos son seleccionados para ser los más representativos de cada clúster.

El segundo bloque muestra el vector de clustering de las observaciones.Cada observación está asignada a un clúster. El vector contiene el número de clúster asignado a cada observación. Como tenemos que  $ k=2$, tenemos dos clústeres en total: el clúster 1 y el clúster 2.

En el tercer bloque vemos los valores de build y swap. Build nos indica el valor de la suma total de las distancias entre las observaciones y los centros de los clústeres. Swap es el valor de la suma total de las distancias después de que el algoritmo intenta mejorar el clustering al intercambiar los centros de los clústers.

El valor de la función objetivo ha disminuido de 4.258150 a 3.916516 después de realizar el cambio de los medoids. Esto indica que el algoritmo ha logrado una mejor asignación de los clústeres al minimizar la suma de las distancias dentro de los clústeres.

Veamos una representación gráfica del clustering dado por la función "pam":

```{r, echo=FALSE}
fviz_cluster(object = medoide_clusters, data = escalado, ellipse.type = "t", repel = TRUE) +
  theme_bw() + theme(legend.position = "none")
```

Veamos si obviando algunas variables podemos encontrar un modelo que tenga mayor representación de variabilidad en las componenetes principales. Debido a la alta relación de las variables podemos ver con la matriz de corelaciones que las varaibles con mas peso son altura,peso y cintura. Veamos si obtenemos un modelo viable solo escogiendo estas variables.

```{r}
mios<-datos %>% select("altura","peso","cintura")
fviz_nbclust(x = mios, FUNcluster = pam, method = "wss",
             diss = dist(datos, method = "manhattan"))
```

Por el método del codo el valor $k=2$ dará los mejores resultados además de evitar solapamientos como antes.

```{r}
medoide_clusters <- pam(x = mios, k = 2, metric = "manhattan")
fviz_cluster(object = medoide_clusters, data = mios, ellipse.type = "t", repel = TRUE) +
  theme_bw() + theme(legend.position = "none")
```
Así obtenemos una variabilidad total de $83.6\%$ con un solapamiento tolerable comparado con los valores de $k$ mayores.

```{r, echo=FALSE}
medoide_clusters
```
Aquí, igual que en el modelo anterior, tenemos una representación de los centros de los clusters. Veamos con la matriz de confusión si los valores estan bien divididos entre mujer y hombre.

La siguiente matriz hace referencia al modelo k-medoide usando la distancia "manhattan".
```{r, echo=FALSE}
datos_1 <- datos %>%
  mutate(cluster = medoide_clusters$clustering)

# Crear tabla de contingencia
tabla_contingencia <- table(datos_1$sexo, datos_1$cluster)

# Ver la tabla
print(tabla_contingencia)

```

Esto nos indica que hay 178 hombres en el clúster 1 y 22 hombres en el clúster 2. Ademas, hay 6 mujeres en el clúster 1 y 194 mujeres en el clúster 2. Como la mayoría de los hombres se encuentran en un clúster y la mayoría de las mujeres en el otro, se puede ver una clara separación entre los sexos en los diferentes clústers. Esto indicaría que el modelo ha logrado identificar diferencias entre sexos y ha agrupado las observaciones en función de estas diferencias.

Por lo tanto podemos deducir que el cluster ha conseguido separar los sexo de una forma bastante buena. Así y todo hay algunos casos minoritarios excepcionales que pueden ser debidos a la diversidad genética y al hecho que pueden existir personas con alturas anormales para lo que estaria asociado a su sexo.